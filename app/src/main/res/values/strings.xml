<resources>
    <string name="app_name">Day12FullyFunctionalAppAssignment</string>
    <string name="main_activity_welcome_message">Welcome ! to Knowledge Hub </string>
    <string name="android_activity_header_message">Android App Development</string>
    <string name="android_activity_content_message">Android is an open-source operating system, based on the Linux kernel and used in mobile devices like smartphones, tablets, etc. Further, it was developed for smartwatches and Android TV. Each of them has a specialized interface. Android has been one of the best-selling OS for smartphones. Android OS was developed by Android Inc. which Google bought in 2005. Various applications like games, music player, camera, etc. are built for these smartphones for running on Android. Google Play Store features quite 3.3 million apps. Today, Android remains dominant on a global scale. Approximately 75% of the world population prefers using Android as against 15% of iOS. It is an operating system that has a huge market for apps. In Android, programming is done in two languages JAVA or Kotlin and XML(Extension Markup Language). The XML file deals with the design, presentation, layouts, blueprint, etc (as a front-end) while the JAVA or KOTLIN deals with working of buttons, variables, storing, etc (as a back-end). And the biggest confusion for an Android beginner is which language to choose between Java and Kotlin? So let me try to overcome the confusion first.Java or Kotlin?Kotlin is the official language for Android App Development Declared by Google  and it is the most used language as well. Many of the apps in the Play Store are built with Kotlin and it is also the most supported language by Google. Kotlin is fast and easy than java. Kotlin also include Lot of new features and libraries that is not present in Java.Java is the native language used by Android, applications that helps to communicate with the operating system and  hardware that directly uses Java. This language allows the creation of any program and supports almost all types of machines, and OS X be it Android, Windows, or Linux.  Java was developed by Sun Microsystems (now the property of Oracle) and one can use Microservices with Java.Kotlin is a cross-platform programming language that can be used as an alternative to Java for Android App Development. It has also been declared  “official” language by google . The only sizable difference is that Kotlin removes one of the features of Java such as null pointer exceptions. It also removes the use of semicolon at the end of every line . In short, Kotlin is much simpler for beginners as  compared to Java and Now most of developer also shift from java to Kotlin for Android App Development.</string>
    <string name="ios_activity_header_message">iOS App Development</string>
    <string name="ios_activity_content_message">In recent years, the mobile app market has seen exponential growth, and there are now millions of apps available for download on both the iOS and Android operating systems. Because of this, businesses require developers that can produce attractive, high-quality apps that stand out in the crowded app market. One of the most coveted among all is iOS developer. The job market has a great demand for qualified iOS developers. iOS development popularly known as “Apple Development”, is so called because the only recruiter of iOS developers is Apple Inc. The world of iOS programming is continually changing, with new tools and features appearing frequently. To remain competitive in the job market, developers must keep up with the most recent advancements and constantly enhance their skills. It can be pleasant and rewarding to combine technical expertise with creative problem-solving while creating an iOS app. All things considered, working as an iOS developer might be a fantastic career choice for those who appreciate programming, solving issues, and utilizing cutting-edge technology. In this engaging and well-paying industry, you can succeed if you have the necessary qualifications and experience.</string>
    <string name="web_activity_header_message">Web App Development</string>
    <string name="web_activity_content_message">Web development refers to the creating, building, and maintaining of websites. It includes aspects such as web design, web publishing, web programming, and database management. It is the creation of an application that works over the internet i.e. websites. Web development is the work involved in developing a website for the Internet (World Wide Web) or an intranet (a private network).[1] Web development can range from developing a simple single static page of plain text to complex web applications, electronic businesses, and social network services. A more comprehensive list of tasks to which Web development commonly refers, may include Web engineering, Web design, Web content development, client liaison, client-side/server-side scripting, Web server and network security configuration, and e-commerce development. Among Web professionals, "Web development" usually refers to the main non-design aspects of building Web sites: writing markup and coding.[2] Web development may use content management systems (CMS) to make content changes easier and available with basic technical skills. For larger organizations and businesses, Web development teams can consist of hundreds of people (Web developers) and follow standard methods like Agile methodologies while developing Web sites.[1] Smaller organizations may only require a single permanent or contracting developer, or secondary assignment to related job positions such as a graphic designer or information systems technician. Web development may be a collaborative effort between departments rather than the domain of a designated department. There are three kinds of Web developer specialization: front-end developer, back-end developer, and full-stack developer.[3] Front-end developers are responsible for behavior and visuals that run in the user browser, while back-end developers deal with the servers.[4] Since the commercialization of the Web, the industry has boomed and has become one of the most used technologies ever.</string>
    <string name="ai_activity_header_message">AI Development</string>
    <string name="ai_activity_content_message">Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.Some high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, Apple Intelligence, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.Alan Turing was the first person to conduct substantial research in the field that he called "machine intelligence".[4] Artificial intelligence was founded as an academic discipline in 1956,[5] by those now considered the founding fathers of AI: John McCarthy, Marvin Minksy, Nathaniel Rochester, and Claude Shannon.[6][7] The field went through multiple cycles of optimism,[8][9] followed by periods of disappointment and loss of funding, known as AI winter.[10][11] Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques,[12] and after 2017 with the transformer architecture.[13] This led to the AI boom of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.[14]The growing use of artificial intelligence in the 21st century is influencing a societal and economic shift towards increased automation, data-driven decision-making, and the integration of AI systems into various economic sectors and areas of life, impacting job markets, healthcare, government, industry, education, propaganda, and disinformation. This raises questions about the long-term effects, ethical implications, and risks of AI, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.</string>
    <string name="ds_activity_header_message">DS Development</string>
    <string name="ds_activity_content_message">Data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processes, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.[2]Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).[3] Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.[4]Data science is "a concept to unify statistics, data analysis, informatics, and their related methods" to "understand and analyze actual phenomena" with data.[5] It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.[6] However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a "fourth paradigm" of science (empirical, theoretical, computational, and now data-driven) and asserted that "everything about science is changing because of the impact of information technology" and the data deluge.[7][8]A data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.[9]Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[11][12] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[13][14] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[15]</string>`
    <string name="ml_activity_header_message">ML Development</string>
    <string name="ml_activity_content_message">Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance. ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the fields methods. The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[12] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[13] Hebbs model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[12] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.By the early 1960s an experimental "learning machine" with punched tape memory, called Cybertron had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively "trained" by a human operator/teacher to recognize patterns and equipped with a "goof" button to cause it to reevaluate incorrect decisions.[14] A representative book on research into machine learning during the 1960s was Nilssons book on Learning Machines, dealing mostly with machine learning for pattern classification.[15] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[16] In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.</string>
</resources>